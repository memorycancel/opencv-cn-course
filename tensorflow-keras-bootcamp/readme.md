# tensorflow深度学习训练营

## 01 神经网络入门

What and Why 神经网络入门视频

<iframe width="1080" height="608" src="https://www.youtube.com/embed/_5XYLA2HLmo" title="Neural Networks - What They Are &amp; Why They Matter - A 30,000 Feet View for Beginners" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

 ![download1.png](01_getting_started_with_neural_networks/download1.png)

本教程面向希望进入机器学习和深度学习领域的绝对初学者。我们将简化许多细节，以便您能够掌握最基本的概念。

### 01-00 目录

Table of Contents

1. 理解神经网络为黑匣子
2. 理解神经网络的输出
3. 理解神经网络的输入
4. 如何理解神经网络训练？

### 01-02 理解神经网络为黑匣子

我们首先将神经网络视为黑匣子；你不知道里面有什么，但正如你在这个例子中看到的，我们有一个任意大小、格式或颜色的输入图像，网络的输出是 0 到 1 之间的三个数字，其中每个输出对应是每种类别的概率：输入图像可以是“猫”、“狗”或其他类别（我们简称为“其他”）。

### 01-03 理解神经网络的输出

我们通常将这些类别称为标签（Labels）或类标签（Class Labels）。这个特殊问题称为图像分类（image classification），其中输入是图像，输出是三个可能类别中每一个类别的可能性数值。需要明确的是网络的输出是三个数值（而不是标签本身）。

在此示例中，网络为第一个输出 0.97，为第二个输出 0.01，为第三个输出 0.02。请注意，三个输出之和为 1，因为它们代表概率。由于第一个输出的概率最高，我们说网络预测输入图像是猫。

一般，分配给输入图像的标签是通过从三个输出中选择与最大概率相关联的标签来计算的。因此，如果输出分别为 0.51、0.48 和 0.01，我们仍会将预测标签分配为 Cat，因为 0.51  仍然代表所有三个类别的最高概率。在这种情况下，网络对预测的信心较低。

如果输入图像是猫，完美神经网络将输出  (1,0,0)；如果输入图像是狗，则输出 (0,1,0)；如果输入图像是猫或狗以外的东西，则最终输出  (0,0,1)。事实上，即使训练有素的网络也无法给出如此完美的结果。实际上，执行图像分类的神经网络可能有数百个可能的类别（不仅仅是三个），但分配类标签的过程是相同的。请记住，神经网络可用于许多其他问题类型，但图像分类是一种非常常见的应用，非常适合作为入门级示例。

### 01-04 理解神经网络的输入

现在让我们看一下神经网络的输入，并考虑如何将这些信息表示为数值。您可能已经知道，灰度图像表示为像素值数组，其中每个像素值代表从纯黑到纯白的强度。

 ![download2.png](01_getting_started_with_neural_networks/download2.png)

彩色图像非常相似，只不过它们的每个像素具有三个分量，分别代表红色、绿色和蓝色的颜色强度。因此，在本例中，256 x 256 彩色图像由 196,608 个数字表示。考虑到这一点，让我们更新我们的图表，以更清楚地反映幕后发生的情况。

 ![download3.png](01_getting_started_with_neural_networks/download3.png)

在这里，我们展示了神经网络期望输入总共有近 200,000  个数字，但我们尚未指定该数据的形状。根据网络的类型，数据可以表示为一维向量；或更紧凑的形式：例如三个二维数组，其中每个数组为  256×256。但无论哪种情况，特定的网络设计都期望数据具有固定的大小和形状。

 ![](01_getting_started_with_neural_networks/download4.png)

这里要注意的是当设计神经网络时，它们为了接受特定大小和形状的输入而完成的。不同的图像分类网络根据其设计支持的应用程序需要不同大小的输入，这并不罕见。例如，由于与移动设备相关的资源有限，为移动设备设计的网络通常需要较小的输入图像。但这没关系，因为我们需要做的就是预处理图像以符合任何特定网络所需的尺寸和形状。

 ![](01_getting_started_with_neural_networks/download5.png)

### 01-05 如何理解神经网络训练？

现在让我们来谈谈如何理解神经网络的训练。关于神经网络要了解的主要内容是它们包含许多可调参数，您可以将其理解为黑匣子上的旋钮设置（在技术术语中，这些旋钮设置称为权重weights）。如果你有这样一个黑匣子，但不知道正确的旋钮设置，它基本上是没有用的，但好消息是，你可以通过有条不紊地训练神经网络来找到正确的设置。

 ![](01_getting_started_with_neural_networks/download6.png) 

训练过程类似于幼儿如何了解周围的世界。在日常生活中，孩子吸收大量的视觉信息，并通过反复试验，在父母的帮助下学会识别世界上的物体。训练神经网络来执行图像分类非常相似。它通常需要大量数据并需要多次迭代才能确定神经网络权重的最佳设置。

当你训练神经网络时，你需要向它展示你希望它学习的各种类别的数千个示例，例如猫的图像、狗的图像以及其他类型物体的图像。这种训练称为监督学习（**supervised learning**），因为您向神经网络提供一个类的图像，并明确告诉图像的类别。

 ![](01_getting_started_with_neural_networks/download7.png) 

下面附上一张，监督学习和非监督学习的图：

 ![supervisor_and_unsuper.jpeg](01_getting_started_with_neural_networks/supervisor_and_unsuper.jpeg) 

如果网络做出错误的预测，我们会计算与错误预测相关的误差，并且该误差用于调整网络中的权重，从而提高后续预测的准确性。

 ![](01_getting_started_with_neural_networks/download8.png) 

在下一个单元中，我们将更深入地研究神经网络的训练方式，包括如何对标记的训练数据进行建模、如何使用损失函数（loss functions ）以及用于更新神经网络权重的技术（称为梯度下降**gradient descent**）。



## 02 训练神经网络的基础知识

<iframe width="899" height="506" src="https://www.youtube.com/embed/4E2_rkP3owI" title="Deep Learning Using Keras – Training Neural Network" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

使用**Keras**进行深度学习，训练神经网络。

Keras是什么？ Keras是**一种用Python编写的高级神经网络应用程序编程接口（API）**，它是开源的。 它建立在CNTK、TensorFlow和Theano等框架之上，旨在通过深度神经网络实现快速实验。 Keras优先考虑其代码的灵活性和适应性，它不处理较低级别的计算，而是将其分配给后端库。

 ![](02_fundamentals_of_training_a_neural_network/download1.png)

在本单元中，我们将介绍针对图像分类问题训练神经网络所需的基本要素。我们仍然将内部网络架构视为黑匣子，以便我们可以专注于训练神经网络所需的其他基本组件和概念。

### 02-00 目录

1. 简介
2. 标记训练数据和 One-Hot 编码
3. 损失函数 Loss functions
4. 梯度下降 Gradient Descent（优化Optimizations）
5. 权重(Weights)更新计算示例
6. 完整的训练循环
7. 训练图(Plots)
8. 使用经过训练的模型进行推理(Inference)
9. 结论

### 02-01 简介

在上一单元中，我们介绍了神经网络的大概全貌，主要关注输入和输出以及如何解释图像分类问题的结果。我们还了解到，神经网络包含必须通过训练过程进行适当调整的权重。在这篇文章中，我们将更深入地研究神经网络的训练方式，而不涉及特定网络架构的细节。这将使我们能够在概念层面上理解模型训练过程。

1. 如何对标记过的（labeled）训练数据进行建模。
2. 如何使用损失函数来量化输入和预测输出之间的误差。
3. 如何使用梯度下降来更新网络中的权重。

### 02-02 标注训练数据和 One-Hot 编码

让我们仔细看看图像分类任务中标记的训练数据是如何表示的。带标签的训练数据由图像及其相应的现实（分类）标签组成。如果网络被设计为对来自三个类别（例如猫、狗、其他）的对象进行分类，我们将需要来自所有三个类别的训练样本。通常每个类别都需要数千个样本。

包含分类标签的数据集可以将标签表示为字符串（“Cat”、“Dog”、“Other”）或整数（0,1,2）。但是，在通过神经网络处理数据集之前，标签必须数字表示。当数据集包含整数标签（例如，0、1、2）来表示类时，会提供一个类标签文件，用于定义从类名称到数据集中的整数表示的映射。这允许在需要时将整数映射回类名。如下所示的类映射。

```text
Label    Description
  0          Cat
  1          Dog
  2          Other
```

这种类型的标签编码称为整数编码，因为使用唯一的整数对类标签进行编码。但是，当类标签之间没有关系时，建议使用One-Hot Encoding。  One-hot  编码是一种将分类标签表示为二进制向量（仅包含零和一）的技术。在此示例中，我们有三个不同的类（猫、狗和其他），因此我们可以使用长度为 3  的向量以数字方式表示每个类，其中其中一个条目为 1，其他条目均为 0。

```text
Cat   Dog  Other
 1     0     0
 0     1     0
 0     0     1
```

顺序是任意的，但它需要在整个数据集中保持一致。

我们首先考虑一个训练样本，如下图所示，它由输入图像和该图像的类标签组成。对于每个输入训练样本，网络将生成一个由三个数字组成的预测，表示输入图像对应于给定类别的概率。概率最高的输出决定了预测结果。在这种情况下，网络（错误地）预测输入图像是“狗”，因为网络的第二个输出具有最高的概率。请注意，网络的输入只是图像。每个输入图像的类标签用于计算损失，如下一节所述。

 ![](02_fundamentals_of_training_a_neural_network/download2.png)

### 02-03 损失函数 Loss Function

所有神经网络都使用**损失函数**来量化给定训练样本的预测输出与真实值之间的**误差**。正如我们将在下一节中看到的，损失函数可用于指导学习过程（即以提高未来预测准确性的方式更新网络权重network weights）。

量化网络输出与预期结果之间的误差的一种方法是计算误差平方和  (`SSE`)，如下所示。这也称为`损失`(`LOSS`)。在下面的示例中，我们通过计算实际标签值与预测标签值的差来计算单个训练样本的误差。然后对每一项进行平方，三项之和代表总误差，在本例中为 0.6638。
$$
SSE=(1−0.37)^2+(0−0.50)^2+(0−0.13)^2=0.6638
$$
在实践中训练神经网络时，在更新网络权重之前，会使用许多图像来计算损失。因此，下一个方程通常用于计算多个训练图像的均方误差 (MSE)。 MSE  只是所有使用的图像的 SSE 的平均值。用于更新权重的图像数量称为批量大小(**batch size**)（批量大小 32  通常是一个很好的默认值）。一批图像的处理称为一次“迭代”（**iteration**）。
$$
\large{\text{MSE} = \frac{1}{n}  \sum_{i=1}^{n}(y_{i} - y^{'}_{i})^2 = \text{mean(SSE)}}
$$
mean表示求平均值。

### 02-04 梯度下降Gradient Descent（优化）

现在我们已经熟悉了损失函数的概念，我们准备好介绍用于更新神经网络中权重的优化过程。幸运的是，有一种方法可以调整神经网络的权重，称为梯度下降(Gradient Descent)。为简单起见，我们将仅使用一个名为 W 的可调参数来说明这个概念，并且假设损失函数是凸函数，因此形状像碗，如图所示。

PS：在学习考研数学和深度学习时我发现凹凸函数网上的叫法不同，在高等数学中下图被叫做凹函数，但是外国教授却叫凸函数（convex function）。这是因为数分和高数对于凸凹函数的定义是反的。

 ![](02_fundamentals_of_training_a_neural_network/download3.png)

损失函数的值显示在垂直轴上，我们的单个可训练权重的值显示在水平轴上。假设当前的权重估计为 
$$
\normalsize{W_{e1}}
$$
参考左图，如果我们计算当前权重估计对应的点处的损失函数的斜率，We1
，我们可以看到斜率（梯度）为负。在这种情况下，我们需要增加权重以接近 Wo 指示的最佳值。所以我们需要沿着与梯度符号相反的方向移动。

另一方面，如果我们当前的权重估计，We1>Wo（如右图所示），梯度将为正，我们需要减少当前权重的值以更接近最佳值Wo。

请注意，在这两种情况下，我们仍然需要沿着与梯度符号相反的方向移动。

在继续之前，请注意，在这两个图中，我们绘制的代表梯度（斜率）的箭头都指向右侧。在一种情况下，箭头指向右下，而在另一种情况下，箭头指向右上。但不要对两个箭头都指向右侧的事实感到困惑，重要的是梯度的符号，

 ![](02_fundamentals_of_training_a_neural_network/download4.png)

请记住，直线的斜率定义为运行过程中的上升，当权重位于最佳值左侧时，函数的斜率为负，而当权重位于最佳值右侧时，函数的斜率为正。所以梯度的符号很重要。

 ![](02_fundamentals_of_training_a_neural_network/download5.png)

在上述两种情况下，我们都需要在与梯度符号相反的方向上调整权重。考虑到这些概念，我们可以证明以下方程可用于在正确的方向上更新权重，而不管权重的当前值相对于最佳值如何。

 ![](02_fundamentals_of_training_a_neural_network/download6.png)

考虑这个问题的最好方法是，梯度的符号决定了我们需要移动的方向。但是我们需要移动的量需要用一个称为学习率（**Learning Rate**）的参数来调整，该参数通常是一个**很小的数字**(小于 1)。学习率是我们在训练之前需要指定的东西，而不是网络学习的东西。像这样的参数通常称为超参数(**hyperparameters**)，以将它们与可训练参数trainable parameters （例如网络权重weights）区分开来。

实际上，损失函数有很多维度，通常不是凸函数，而是有很多峰和谷。一般情况下，损失函数的斜率称为梯度，是网络中所有权重的函数。但用于更新权重的方法在概念上与此处描述的相同。

  ![](02_fundamentals_of_training_a_neural_network/download7.png)

### 02-05 权重更新计算示例

为了使这一点更加具体，让我们进行一个更新权重的示例计算。这里，我们假设当前权重为 We1，其值为 0.38。我们还将假设学习率为  0.01，并且损失函数在 We1 点的斜率等于 -0.55。使用上面的更新方程，我们可以轻松计算出新的权重估计值，我们将其称为  We2。这个计算得到了简化，因为我们只在一个维度上工作，很容易扩展到多个维度。

PS: *A gradient is a vector, and slope is a scalar*. 上面算的是一维的，所以就是slope，扩展到多个维度就是gradient梯度，如上图所示，所以梯度是一个多维的坡度，而slope斜率是一维平坡。

  ![](02_fundamentals_of_training_a_neural_network/download8.png)

~~我们还没有讨论的一件事是如何实际计算损失函数相对于网络权重的梯度。~~幸运的是，这是通过一种称为反向传播**backpropagation**的算法来处理的，该算法内置于深度学习框架中，例如 TensorFlow、Keras 和 PyTorch，因此您不需要自己实现。

### 02-06 完整的训练闭环

现在我们已经涵盖了与训练神经网络相关的所有基本要素，我们可以在下图中总结该过程。

  ![](02_fundamentals_of_training_a_neural_network/download9.png)

这里，左边是输入图像，右边是网络的输出，我们将其称为 y′。我们使用真实标签 y 以及网络的预测输出来计算损失。请注意，我们没有具体显示网络的多个输出，但应该理解的是 y′ 和 y是向量，其长度等于网络正在训练的类的数量。

计算损失后，我们可以计算损失相对于权重的梯度，然后可以将其用于更新网络中的权重。**这是一个重要的图表，它高度概括了神经网络的训练过程。**

### 02-07 训练曲线 Plots

现在我们已经知道如何更新网络中的权重，值得强调的是，训练神经网络是一个迭代过程，通常需要将整个训练集多次通过网络。

每次整个训练数据集通过网络时，我们将其称为训练时期（**training epoch**）。训练神经网络通常需要许多训练周期，直到损失随着额外训练而停止减少。正如您在下面的第一张图中所看到的，随着训练的进行，损失减少的速度逐渐减小，这表明模型正在接近其学习能力。

  ![](02_fundamentals_of_training_a_neural_network/download10.png)

绘制训练准确度图也很常见，正如您所期望的那样，随着损失的减少，准确度往往会增加，如第二张图所示。

  ![](02_fundamentals_of_training_a_neural_network/download11.png)

有许多与训练神经网络相关的重要细节，我们在第一篇文章中没有介绍，但随着本系列的进展，我们将继续介绍有关该主题的更多高级概念。

注意：我们尚未讨论的一个重要主题是数据分割（**data splitting**）。这涉及到验证数据集的概念，**用于在训练过程中评估训练模型的质量（正确率）**。这是一个重要且核心的主题，将在后续文章中介绍。

### 02-08 使用经过训练的模型执行推理

现在我们已经介绍了如何训练神经网络的过程，有必要谈谈我们将如何使用它。一旦我们有了训练有素的网络，我们就可以为其提供未知内容的图像，并使用该网络来预测该图像属于哪个类别。这反映在下图中，我们所需要的只是想要分类的未知内容的图像。对未知数据进行预测（make prediction）通常称为使用网络进行推理（perform inference）。

 ![](02_fundamentals_of_training_a_neural_network/download12.png)

### 02-09 结论

让我们总结一下与训练神经网络相关的要点。

+ 训练神经网络以执行图像分类等监督学习任务需要标记过的训练数据(labled)。
+ 在大多数情况下，建议对分类数据使用 One-Hot 标签编码。（[1,0,0],[0,1,0],[0,0,1]）
+ 训练神经网络需要一个损失函数，用于量化网络输出和预期输出之间的误差。
+ 损失函数的梯度是使用称为反向传播backpropagation的算法计算的，该算法内置于 TensorFlow 和 PyTorch 等深度学习框架中。
+ 梯度下降以迭代方式用于更新神经网络的权重。
+ 训练图像的子集（批量大小batch size）用于执行权重更新。这称为训练时期内epoch的迭代。
+ 训练时期包括通过网络处理整个训练数据集。因此，训练时期的迭代次数等于训练图像的数量除以批量大小。
+ 每个训练周期代表训练过程的完整过程，直到损失函数稳定。注意：在实践中，我们不仅仅依靠训练损失来评估训练模型的质量。还需要验证损失，我们将在后续文章中介绍这一点。

